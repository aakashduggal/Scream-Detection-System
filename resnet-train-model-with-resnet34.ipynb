{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-12-05T08:25:57.340894Z",
     "iopub.status.busy": "2023-12-05T08:25:57.340000Z",
     "iopub.status.idle": "2023-12-05T08:25:57.345898Z",
     "shell.execute_reply": "2023-12-05T08:25:57.344986Z",
     "shell.execute_reply.started": "2023-12-05T08:25:57.340862Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def count_files_in_directory(directory):\n",
    "    return len([name for name in os.listdir(directory) if os.path.isfile(os.path.join(directory, name))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T08:25:59.894585Z",
     "iopub.status.busy": "2023-12-05T08:25:59.893643Z",
     "iopub.status.idle": "2023-12-05T08:26:02.575097Z",
     "shell.execute_reply": "2023-12-05T08:26:02.574139Z",
     "shell.execute_reply.started": "2023-12-05T08:25:59.894533Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in '/kaggle/input/transform-and-pre-progress-for-audio/Data/Images/not/': 2629\n"
     ]
    }
   ],
   "source": [
    "directory_path = '/kaggle/input/transform-and-pre-progress-for-audio/Data/Images/not/'\n",
    "file_count = count_files_in_directory(directory_path)\n",
    "print(f\"Number of files in '{directory_path}': {file_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T08:26:03.472933Z",
     "iopub.status.busy": "2023-12-05T08:26:03.472178Z",
     "iopub.status.idle": "2023-12-05T08:26:03.644587Z",
     "shell.execute_reply": "2023-12-05T08:26:03.643561Z",
     "shell.execute_reply.started": "2023-12-05T08:26:03.472883Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in '/kaggle/input/transform-and-pre-progress-for-audio/Data/Images/scream/': 909\n"
     ]
    }
   ],
   "source": [
    "directory_path = '/kaggle/input/transform-and-pre-progress-for-audio/Data/Images/scream/'\n",
    "file_count = count_files_in_directory(directory_path)\n",
    "print(f\"Number of files in '{directory_path}': {file_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We import the needpackage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T08:26:07.537036Z",
     "iopub.status.busy": "2023-12-05T08:26:07.536324Z",
     "iopub.status.idle": "2023-12-05T08:26:07.541153Z",
     "shell.execute_reply": "2023-12-05T08:26:07.540284Z",
     "shell.execute_reply.started": "2023-12-05T08:26:07.536992Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we make the dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define datalo0ader by folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T08:26:10.120944Z",
     "iopub.status.busy": "2023-12-05T08:26:10.120162Z",
     "iopub.status.idle": "2023-12-05T08:26:11.017047Z",
     "shell.execute_reply": "2023-12-05T08:26:11.016137Z",
     "shell.execute_reply.started": "2023-12-05T08:26:10.120895Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3538"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = '/kaggle/input/transform-and-pre-progress-for-audio/Data/Images/' #looking in subfolder train\n",
    "\n",
    "scream_dataset = datasets.ImageFolder(\n",
    "    root=data_path,\n",
    "    transform=transforms.Compose([transforms.Resize((64,862)),\n",
    "                                  transforms.ToTensor()])\n",
    ")\n",
    "\n",
    "len(scream_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check label meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T08:26:11.607476Z",
     "iopub.status.busy": "2023-12-05T08:26:11.606323Z",
     "iopub.status.idle": "2023-12-05T08:26:11.612740Z",
     "shell.execute_reply": "2023-12-05T08:26:11.611733Z",
     "shell.execute_reply.started": "2023-12-05T08:26:11.607432Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class category and index of the images: {'not': 0, 'scream': 1}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class_map=scream_dataset.class_to_idx\n",
    "\n",
    "print(\"\\nClass category and index of the images: {}\\n\".format(class_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define trainset and testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T08:26:13.047244Z",
     "iopub.status.busy": "2023-12-05T08:26:13.046462Z",
     "iopub.status.idle": "2023-12-05T08:26:13.058072Z",
     "shell.execute_reply": "2023-12-05T08:26:13.057042Z",
     "shell.execute_reply.started": "2023-12-05T08:26:13.047208Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 2830\n",
      "Testing size: 708\n"
     ]
    }
   ],
   "source": [
    "#split data to test and train\n",
    "train_size = int(0.8 * len(scream_dataset)) # We use 80% as train\n",
    "test_size = len(scream_dataset) - train_size\n",
    "scream_train_dataset, scream_test_dataset = torch.utils.data.random_split(scream_dataset, [train_size, test_size])\n",
    "\n",
    "print(\"Training size:\", len(scream_train_dataset))\n",
    "print(\"Testing size:\",len(scream_test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the sample count in trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T08:26:14.881253Z",
     "iopub.status.busy": "2023-12-05T08:26:14.880875Z",
     "iopub.status.idle": "2023-12-05T08:26:34.730346Z",
     "shell.execute_reply": "2023-12-05T08:26:34.729157Z",
     "shell.execute_reply.started": "2023-12-05T08:26:14.881216Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 2088, 1: 742})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# labels in training set\n",
    "train_classes = [label for _, label in scream_train_dataset]\n",
    "Counter(train_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define trainloader and testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T08:26:55.022809Z",
     "iopub.status.busy": "2023-12-05T08:26:55.022375Z",
     "iopub.status.idle": "2023-12-05T08:26:55.028340Z",
     "shell.execute_reply": "2023-12-05T08:26:55.027369Z",
     "shell.execute_reply.started": "2023-12-05T08:26:55.022778Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    scream_train_dataset,\n",
    "    batch_size=64,\n",
    "    num_workers=2,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    scream_test_dataset,\n",
    "    batch_size=64,\n",
    "    num_workers=2,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View a example image shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T08:26:58.570175Z",
     "iopub.status.busy": "2023-12-05T08:26:58.569263Z",
     "iopub.status.idle": "2023-12-05T08:26:58.582627Z",
     "shell.execute_reply": "2023-12-05T08:26:58.581709Z",
     "shell.execute_reply.started": "2023-12-05T08:26:58.570139Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64, 862])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td = train_dataloader.dataset[0][0]\n",
    "td.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use resnbet34 for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T08:27:00.543039Z",
     "iopub.status.busy": "2023-12-05T08:27:00.542297Z",
     "iopub.status.idle": "2023-12-05T08:27:01.007881Z",
     "shell.execute_reply": "2023-12-05T08:27:01.007098Z",
     "shell.execute_reply.started": "2023-12-05T08:27:00.543005Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import resnet34, ResNet34_Weights\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')\n",
    "\n",
    "# Updated model loading with weights\n",
    "model = resnet34(weights=ResNet34_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Updating the fully connected layer and the first convolutional layer\n",
    "model.fc = nn.Linear(512, 2)\n",
    "model.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T08:29:47.977682Z",
     "iopub.status.busy": "2023-12-05T08:29:47.976773Z",
     "iopub.status.idle": "2023-12-05T08:29:47.982857Z",
     "shell.execute_reply": "2023-12-05T08:29:47.981962Z",
     "shell.execute_reply.started": "2023-12-05T08:29:47.977648Z"
    }
   },
   "outputs": [],
   "source": [
    "# cost function used to determine best parameters\n",
    "cost = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# used to create optimal parameters\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Train and Test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T08:28:29.413464Z",
     "iopub.status.busy": "2023-12-05T08:28:29.413079Z",
     "iopub.status.idle": "2023-12-05T08:28:29.425223Z",
     "shell.execute_reply": "2023-12-05T08:28:29.423984Z",
     "shell.execute_reply.started": "2023-12-05T08:28:29.413435Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the training function\n",
    "\n",
    "def train(dataloader, model, loss, optimizer):\n",
    "    model.train()\n",
    "    size = len(dataloader.dataset)\n",
    "    correct = 0  # Counter for correct predictions\n",
    "    total = 0  # Counter for total examples\n",
    "\n",
    "    for batch, (X, Y) in enumerate(dataloader):\n",
    "\n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X)\n",
    "        loss = cost(pred, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute accuracy\n",
    "        _, predicted = pred.max(1)\n",
    "        total += Y.size(0)\n",
    "        correct += predicted.eq(Y).sum().item()\n",
    "\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f'loss: {loss:>7f}  [{current:>5d}/{size:>5d}]  Train Accuracy: {(100 * correct / total):.2f}%')\n",
    "\n",
    "\n",
    "# Create the validation/test function\n",
    "\n",
    "def test(dataloader, model):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, Y) in enumerate(dataloader):\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            pred = model(X)\n",
    "\n",
    "            test_loss += cost(pred, Y).item()\n",
    "            correct += (pred.argmax(1)==Y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "\n",
    "    print(f'\\nTest Error:\\nacc: {(100*correct):>0.1f}%, avg loss: {test_loss:>8f}\\n')\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T08:29:51.144502Z",
     "iopub.status.busy": "2023-12-05T08:29:51.144131Z",
     "iopub.status.idle": "2023-12-05T08:36:30.597526Z",
     "shell.execute_reply": "2023-12-05T08:36:30.596341Z",
     "shell.execute_reply.started": "2023-12-05T08:29:51.144474Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.796703  [    0/ 2830]  Train Accuracy: 35.94%\n",
      "loss: 0.407079  [  640/ 2830]  Train Accuracy: 69.46%\n",
      "loss: 0.487745  [ 1280/ 2830]  Train Accuracy: 75.45%\n",
      "loss: 0.341374  [ 1920/ 2830]  Train Accuracy: 77.12%\n",
      "loss: 0.490988  [ 2560/ 2830]  Train Accuracy: 77.21%\n",
      "\n",
      "Test Error:\n",
      "acc: 82.9%, avg loss: 0.007414\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.327747  [    0/ 2830]  Train Accuracy: 90.62%\n",
      "loss: 0.529912  [  640/ 2830]  Train Accuracy: 84.09%\n",
      "loss: 0.487397  [ 1280/ 2830]  Train Accuracy: 82.37%\n",
      "loss: 0.371477  [ 1920/ 2830]  Train Accuracy: 83.37%\n",
      "loss: 0.375720  [ 2560/ 2830]  Train Accuracy: 83.04%\n",
      "\n",
      "Test Error:\n",
      "acc: 86.3%, avg loss: 0.005908\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.359630  [    0/ 2830]  Train Accuracy: 81.25%\n",
      "loss: 0.430460  [  640/ 2830]  Train Accuracy: 84.38%\n",
      "loss: 0.398468  [ 1280/ 2830]  Train Accuracy: 84.23%\n",
      "loss: 0.278411  [ 1920/ 2830]  Train Accuracy: 84.78%\n",
      "loss: 0.358233  [ 2560/ 2830]  Train Accuracy: 84.57%\n",
      "\n",
      "Test Error:\n",
      "acc: 85.5%, avg loss: 0.006908\n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.296660  [    0/ 2830]  Train Accuracy: 85.94%\n",
      "loss: 0.314952  [  640/ 2830]  Train Accuracy: 86.65%\n",
      "loss: 0.464851  [ 1280/ 2830]  Train Accuracy: 86.38%\n",
      "loss: 0.360333  [ 1920/ 2830]  Train Accuracy: 85.89%\n",
      "loss: 0.232413  [ 2560/ 2830]  Train Accuracy: 86.24%\n",
      "\n",
      "Test Error:\n",
      "acc: 76.4%, avg loss: 0.012573\n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.367590  [    0/ 2830]  Train Accuracy: 82.81%\n",
      "loss: 0.180322  [  640/ 2830]  Train Accuracy: 86.51%\n",
      "loss: 0.267568  [ 1280/ 2830]  Train Accuracy: 86.90%\n",
      "loss: 0.248854  [ 1920/ 2830]  Train Accuracy: 87.15%\n",
      "loss: 0.242448  [ 2560/ 2830]  Train Accuracy: 87.31%\n",
      "\n",
      "Test Error:\n",
      "acc: 87.9%, avg loss: 0.005858\n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.270293  [    0/ 2830]  Train Accuracy: 89.06%\n",
      "loss: 0.366659  [  640/ 2830]  Train Accuracy: 86.93%\n",
      "loss: 0.324081  [ 1280/ 2830]  Train Accuracy: 86.68%\n",
      "loss: 0.259250  [ 1920/ 2830]  Train Accuracy: 87.05%\n",
      "loss: 0.229580  [ 2560/ 2830]  Train Accuracy: 87.31%\n",
      "\n",
      "Test Error:\n",
      "acc: 88.6%, avg loss: 0.005925\n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.444900  [    0/ 2830]  Train Accuracy: 81.25%\n",
      "loss: 0.155762  [  640/ 2830]  Train Accuracy: 87.78%\n",
      "loss: 0.317498  [ 1280/ 2830]  Train Accuracy: 88.99%\n",
      "loss: 0.234614  [ 1920/ 2830]  Train Accuracy: 88.56%\n",
      "loss: 0.191173  [ 2560/ 2830]  Train Accuracy: 89.44%\n",
      "\n",
      "Test Error:\n",
      "acc: 89.8%, avg loss: 0.004143\n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.108771  [    0/ 2830]  Train Accuracy: 95.31%\n",
      "loss: 0.186133  [  640/ 2830]  Train Accuracy: 92.05%\n",
      "loss: 0.172717  [ 1280/ 2830]  Train Accuracy: 91.82%\n",
      "loss: 0.333463  [ 1920/ 2830]  Train Accuracy: 91.08%\n",
      "loss: 0.221052  [ 2560/ 2830]  Train Accuracy: 90.93%\n",
      "\n",
      "Test Error:\n",
      "acc: 88.7%, avg loss: 0.006691\n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.122748  [    0/ 2830]  Train Accuracy: 96.88%\n",
      "loss: 0.295028  [  640/ 2830]  Train Accuracy: 90.20%\n",
      "loss: 0.212830  [ 1280/ 2830]  Train Accuracy: 90.03%\n",
      "loss: 0.126313  [ 1920/ 2830]  Train Accuracy: 90.02%\n",
      "loss: 0.237132  [ 2560/ 2830]  Train Accuracy: 89.94%\n",
      "\n",
      "Test Error:\n",
      "acc: 84.9%, avg loss: 0.006330\n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.121304  [    0/ 2830]  Train Accuracy: 95.31%\n",
      "loss: 0.432361  [  640/ 2830]  Train Accuracy: 91.05%\n",
      "loss: 0.236440  [ 1280/ 2830]  Train Accuracy: 91.96%\n",
      "loss: 0.255140  [ 1920/ 2830]  Train Accuracy: 91.53%\n",
      "loss: 0.147407  [ 2560/ 2830]  Train Accuracy: 91.46%\n",
      "\n",
      "Test Error:\n",
      "acc: 89.5%, avg loss: 0.006209\n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.178889  [    0/ 2830]  Train Accuracy: 89.06%\n",
      "loss: 0.411802  [  640/ 2830]  Train Accuracy: 89.49%\n",
      "loss: 0.356503  [ 1280/ 2830]  Train Accuracy: 88.24%\n",
      "loss: 0.213612  [ 1920/ 2830]  Train Accuracy: 89.62%\n",
      "loss: 0.218126  [ 2560/ 2830]  Train Accuracy: 90.05%\n",
      "\n",
      "Test Error:\n",
      "acc: 88.4%, avg loss: 0.005295\n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.241577  [    0/ 2830]  Train Accuracy: 87.50%\n",
      "loss: 0.138941  [  640/ 2830]  Train Accuracy: 89.91%\n",
      "loss: 0.224401  [ 1280/ 2830]  Train Accuracy: 90.40%\n",
      "loss: 0.186005  [ 1920/ 2830]  Train Accuracy: 90.78%\n",
      "loss: 0.279120  [ 2560/ 2830]  Train Accuracy: 90.51%\n",
      "\n",
      "Test Error:\n",
      "acc: 86.9%, avg loss: 0.006910\n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.151492  [    0/ 2830]  Train Accuracy: 95.31%\n",
      "loss: 0.210260  [  640/ 2830]  Train Accuracy: 93.18%\n",
      "loss: 0.123847  [ 1280/ 2830]  Train Accuracy: 93.97%\n",
      "loss: 0.129859  [ 1920/ 2830]  Train Accuracy: 93.95%\n",
      "loss: 0.272646  [ 2560/ 2830]  Train Accuracy: 93.03%\n",
      "\n",
      "Test Error:\n",
      "acc: 84.7%, avg loss: 0.007446\n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.300575  [    0/ 2830]  Train Accuracy: 90.62%\n",
      "loss: 0.252404  [  640/ 2830]  Train Accuracy: 92.19%\n",
      "loss: 0.258732  [ 1280/ 2830]  Train Accuracy: 92.11%\n",
      "loss: 0.195857  [ 1920/ 2830]  Train Accuracy: 92.19%\n",
      "loss: 0.156583  [ 2560/ 2830]  Train Accuracy: 92.76%\n",
      "\n",
      "Test Error:\n",
      "acc: 89.3%, avg loss: 0.005668\n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.082865  [    0/ 2830]  Train Accuracy: 96.88%\n",
      "loss: 0.171502  [  640/ 2830]  Train Accuracy: 94.74%\n",
      "loss: 0.186892  [ 1280/ 2830]  Train Accuracy: 94.20%\n",
      "loss: 0.219621  [ 1920/ 2830]  Train Accuracy: 94.25%\n",
      "loss: 0.073849  [ 2560/ 2830]  Train Accuracy: 94.44%\n",
      "\n",
      "Test Error:\n",
      "acc: 89.4%, avg loss: 0.007027\n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.048198  [    0/ 2830]  Train Accuracy: 98.44%\n",
      "loss: 0.200943  [  640/ 2830]  Train Accuracy: 91.05%\n",
      "loss: 0.221134  [ 1280/ 2830]  Train Accuracy: 91.29%\n",
      "loss: 0.123243  [ 1920/ 2830]  Train Accuracy: 91.58%\n",
      "loss: 0.174485  [ 2560/ 2830]  Train Accuracy: 91.65%\n",
      "\n",
      "Test Error:\n",
      "acc: 86.7%, avg loss: 0.006275\n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.147172  [    0/ 2830]  Train Accuracy: 92.19%\n",
      "loss: 0.107595  [  640/ 2830]  Train Accuracy: 95.74%\n",
      "loss: 0.176467  [ 1280/ 2830]  Train Accuracy: 94.72%\n",
      "loss: 0.095433  [ 1920/ 2830]  Train Accuracy: 94.61%\n",
      "loss: 0.231069  [ 2560/ 2830]  Train Accuracy: 94.25%\n",
      "\n",
      "Test Error:\n",
      "acc: 89.7%, avg loss: 0.006776\n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.100928  [    0/ 2830]  Train Accuracy: 95.31%\n",
      "loss: 0.048887  [  640/ 2830]  Train Accuracy: 96.88%\n",
      "loss: 0.149932  [ 1280/ 2830]  Train Accuracy: 96.28%\n",
      "loss: 0.081799  [ 1920/ 2830]  Train Accuracy: 96.07%\n",
      "loss: 0.211699  [ 2560/ 2830]  Train Accuracy: 95.54%\n",
      "\n",
      "Test Error:\n",
      "acc: 87.3%, avg loss: 0.007200\n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.208140  [    0/ 2830]  Train Accuracy: 92.19%\n",
      "loss: 0.117922  [  640/ 2830]  Train Accuracy: 95.45%\n",
      "loss: 0.099063  [ 1280/ 2830]  Train Accuracy: 94.87%\n",
      "loss: 0.086074  [ 1920/ 2830]  Train Accuracy: 95.01%\n",
      "loss: 0.245138  [ 2560/ 2830]  Train Accuracy: 95.12%\n",
      "\n",
      "Test Error:\n",
      "acc: 87.9%, avg loss: 0.006022\n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.048148  [    0/ 2830]  Train Accuracy: 98.44%\n",
      "loss: 0.094594  [  640/ 2830]  Train Accuracy: 97.73%\n",
      "loss: 0.023499  [ 1280/ 2830]  Train Accuracy: 97.47%\n",
      "loss: 0.090770  [ 1920/ 2830]  Train Accuracy: 97.23%\n",
      "loss: 0.065746  [ 2560/ 2830]  Train Accuracy: 97.26%\n",
      "\n",
      "Test Error:\n",
      "acc: 89.4%, avg loss: 0.005749\n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.079188  [    0/ 2830]  Train Accuracy: 95.31%\n",
      "loss: 0.023051  [  640/ 2830]  Train Accuracy: 96.02%\n",
      "loss: 0.075473  [ 1280/ 2830]  Train Accuracy: 96.50%\n",
      "loss: 0.025711  [ 1920/ 2830]  Train Accuracy: 96.72%\n",
      "loss: 0.144158  [ 2560/ 2830]  Train Accuracy: 96.23%\n",
      "\n",
      "Test Error:\n",
      "acc: 91.5%, avg loss: 0.005219\n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.043072  [    0/ 2830]  Train Accuracy: 100.00%\n",
      "loss: 0.124542  [  640/ 2830]  Train Accuracy: 97.73%\n",
      "loss: 0.094634  [ 1280/ 2830]  Train Accuracy: 97.77%\n",
      "loss: 0.073010  [ 1920/ 2830]  Train Accuracy: 96.82%\n",
      "loss: 0.192094  [ 2560/ 2830]  Train Accuracy: 96.91%\n",
      "\n",
      "Test Error:\n",
      "acc: 91.0%, avg loss: 0.005415\n",
      "\n",
      "Early stopping triggered. No improvement in test loss for 15 epochs.\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Define the early stopping parameters\n",
    "early_stopping_patience = 15  # Number of epochs to wait before early stopping\n",
    "best_loss = torch.inf\n",
    "wait = 0  # Counter for patience\n",
    "epochs = 150\n",
    "\n",
    "best_model_weights = None\n",
    "\n",
    "# Training loop\n",
    "for t in range(epochs):\n",
    "    print(f'Epoch {t + 1}\\n-------------------------------')\n",
    "    train(train_dataloader, model, cost, optimizer)\n",
    "    test_loss = test(test_dataloader, model)\n",
    "\n",
    "    # Check if the test loss has improved\n",
    "    if test_loss < best_loss:\n",
    "        best_loss = test_loss\n",
    "        wait = 0  # Reset patience\n",
    "\n",
    "        # Save the best model weights\n",
    "        best_model_weights = model.state_dict()\n",
    "    else:\n",
    "        wait += 1\n",
    "\n",
    "    if wait >= early_stopping_patience:\n",
    "        print(\"Early stopping triggered. No improvement in test loss for {} epochs.\".format(early_stopping_patience))\n",
    "        break  # Stop training\n",
    "\n",
    "# Restore the best model weights\n",
    "if best_model_weights is not None:\n",
    "    model.load_state_dict(best_model_weights)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T08:37:20.676546Z",
     "iopub.status.busy": "2023-12-05T08:37:20.675614Z",
     "iopub.status.idle": "2023-12-05T08:37:20.879316Z",
     "shell.execute_reply": "2023-12-05T08:37:20.878417Z",
     "shell.execute_reply.started": "2023-12-05T08:37:20.676508Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as Resnet34_Model_2023-12-05--08-37-20.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "# Get the current timestamp in the desired format\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d--%H-%M-%S\")\n",
    "\n",
    "# Define the file name with the timestamp\n",
    "file_name = f\"Resnet34_Model_{timestamp}.pt\"\n",
    "\n",
    "# Save the entire model (including architecture and weights)\n",
    "torch.save(model, file_name)\n",
    "\n",
    "# Print the saved file name\n",
    "print(f\"Model saved as {file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's see how the model can be used!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define output direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T08:53:49.951137Z",
     "iopub.status.busy": "2023-12-05T08:53:49.950230Z",
     "iopub.status.idle": "2023-12-05T08:53:49.958631Z",
     "shell.execute_reply": "2023-12-05T08:53:49.957662Z",
     "shell.execute_reply.started": "2023-12-05T08:53:49.951099Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory '/kaggle/working/transform-and-pre-progress-for-audio/Data/TestImages' already exists\n",
      "Directory '/kaggle/working/transform-and-pre-progress-for-audio/Data/TestImages/Screaming' already exists\n",
      "Directory '/kaggle/working/transform-and-pre-progress-for-audio/Data/TestImages/NotScreaming' already exists\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Directory path\n",
    "directory = \"/kaggle/working/transform-and-pre-progress-for-audio/Data/TestImages\"\n",
    "\n",
    "# Check if the directory already exists\n",
    "if not os.path.exists(directory):\n",
    "    # Create the directory\n",
    "    os.makedirs(directory)\n",
    "    print(f\"Directory '{directory}' created\")\n",
    "else:\n",
    "    print(f\"Directory '{directory}' already exists\")\n",
    "\n",
    "    # Folder names to create\n",
    "folders = [\"Screaming\", \"NotScreaming\"]\n",
    "\n",
    "# Create each folder\n",
    "for folder in folders:\n",
    "    directory_path = os.path.join(directory, folder)\n",
    "\n",
    "    # Check if the directory already exists\n",
    "    if not os.path.exists(directory_path):\n",
    "        # Create the directory\n",
    "        os.makedirs(directory_path)\n",
    "        print(f\"Directory '{directory_path}' created\")\n",
    "    else:\n",
    "        print(f\"Directory '{directory_path}' already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the function apply to audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T08:49:40.035508Z",
     "iopub.status.busy": "2023-12-05T08:49:40.035164Z",
     "iopub.status.idle": "2023-12-05T08:49:40.044851Z",
     "shell.execute_reply": "2023-12-05T08:49:40.043740Z",
     "shell.execute_reply.started": "2023-12-05T08:49:40.035484Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torchaudio\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def pad_waveform(waveform, target_length):\n",
    "    num_channels, current_length = waveform.shape\n",
    "\n",
    "    if current_length < target_length:\n",
    "        # Calculate the amount of padding needed\n",
    "        padding = target_length - current_length\n",
    "        # Pad the waveform with zeros on the right side\n",
    "        waveform = torch.nn.functional.pad(waveform, (0, padding))\n",
    "\n",
    "    return waveform\n",
    "\n",
    "# Define a function to transform audio data into images\n",
    "def transform_data_to_image(audio, sample_rate, label, i):\n",
    "    # Pad waveform to a consistent length of 44100 samples\n",
    "    audio = pad_waveform(audio, 441000)\n",
    "\n",
    "    spectrogram_tensor = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate, n_mels=64, n_fft=1024)(audio)[0] + 1e-10\n",
    "\n",
    "    # Save the spectrogram as an image\n",
    "    image_path = f'/kaggle/working/transform-and-pre-progress-for-audio/Data/TestImages/{label}/audio_img{i}.png'\n",
    "\n",
    "    plt.imsave(image_path, spectrogram_tensor.log2().numpy(), cmap='viridis')\n",
    "    return image_path\n",
    "\n",
    "# Define the image transformation pipeline\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 862)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x[:3, :, :])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform and predict the wav for positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Thus we use the train set as example show, you can use any wave audio within 10s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T08:53:56.914658Z",
     "iopub.status.busy": "2023-12-05T08:53:56.914291Z",
     "iopub.status.idle": "2023-12-05T08:55:03.019446Z",
     "shell.execute_reply": "2023-12-05T08:55:03.018475Z",
     "shell.execute_reply.started": "2023-12-05T08:53:56.914629Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nIFbKv1qjfw_out.wav</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d4v3_z0ISrM_out.wav</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9AZZncb_yek_out.wav</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IdenFdkeASo_out.wav</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LY90s5AgkWM_out.wav</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>857</th>\n",
       "      <td>nmbLZtYRoBs_out.wav</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>858</th>\n",
       "      <td>OtTKt5--3jo_out.wav</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>_zzhHu7HwZc_out.wav</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>ZPAY71_lrEk_out.wav</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>fEgkNrEcwE4_out.wav</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>862 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Filename  Prediction\n",
       "0    nIFbKv1qjfw_out.wav           1\n",
       "1    d4v3_z0ISrM_out.wav           1\n",
       "2    9AZZncb_yek_out.wav           1\n",
       "3    IdenFdkeASo_out.wav           1\n",
       "4    LY90s5AgkWM_out.wav           1\n",
       "..                   ...         ...\n",
       "857  nmbLZtYRoBs_out.wav           1\n",
       "858  OtTKt5--3jo_out.wav           1\n",
       "859  _zzhHu7HwZc_out.wav           1\n",
       "860  ZPAY71_lrEk_out.wav           1\n",
       "861  fEgkNrEcwE4_out.wav           1\n",
       "\n",
       "[862 rows x 2 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the folder containing WAV files\n",
    "folder_path = '/kaggle/input/human-screaming-detection-dataset/Screaming'  # Replace with the path to your folder\n",
    "label = 'Screaming'  # Label for the images\n",
    "\n",
    "# Create an empty list to store data\n",
    "predictions_data = []\n",
    "\n",
    "# Iterate through WAV files in the folder\n",
    "for i, filename in enumerate(os.listdir(folder_path)):\n",
    "    if filename.endswith('.wav'):\n",
    "        # Load the audio\n",
    "        audio, sample_rate = torchaudio.load(os.path.join(folder_path, filename))\n",
    "\n",
    "        # Transform audio to an image and save it\n",
    "        image_path = transform_data_to_image(audio, sample_rate, label, i)\n",
    "\n",
    "        # Load the saved image and apply transformations\n",
    "        image = Image.open(image_path)\n",
    "        image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "        # Make predictions using the model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(image.to(device))\n",
    "\n",
    "        predict = outputs.argmax(dim=1).cpu().detach().numpy().ravel()[0]\n",
    "\n",
    "        # Store the filename and prediction in the DataFrame\n",
    "        predictions_data.append({'Filename': filename, 'Prediction': predict})\n",
    "\n",
    "# Create a DataFrame from the list of data\n",
    "scream_predictions_df = pd.DataFrame(predictions_data)\n",
    "\n",
    "# Display the DataFrame with predictions\n",
    "scream_predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T08:55:15.777626Z",
     "iopub.status.busy": "2023-12-05T08:55:15.776836Z",
     "iopub.status.idle": "2023-12-05T08:55:15.791496Z",
     "shell.execute_reply": "2023-12-05T08:55:15.790523Z",
     "shell.execute_reply.started": "2023-12-05T08:55:15.777591Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction\n",
       "1    776\n",
       "0     86\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scream_predictions_df['Prediction'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform and predict the wav for negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T08:56:28.579523Z",
     "iopub.status.busy": "2023-12-05T08:56:28.578860Z",
     "iopub.status.idle": "2023-12-05T09:00:01.833755Z",
     "shell.execute_reply": "2023-12-05T09:00:01.832720Z",
     "shell.execute_reply.started": "2023-12-05T08:56:28.579486Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lcgKlZ1xFGk_out.wav</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OqXsRZMZQDY_out.wav</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cRQeAJabx0c_out.wav</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>87IhSY4r2DY_out.wav</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BI4fjQBZliY_out.wav</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2626</th>\n",
       "      <td>Obc79LFQ05Q_out.wav</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2627</th>\n",
       "      <td>bc39DoAFVe0_out.wav</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2628</th>\n",
       "      <td>7K4kLUy6Kqo_out.wav</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2629</th>\n",
       "      <td>pk0xBpBk0s0_out.wav</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2630</th>\n",
       "      <td>XY94GCgWMZM_out.wav</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2631 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Filename  Prediction\n",
       "0     lcgKlZ1xFGk_out.wav           0\n",
       "1     OqXsRZMZQDY_out.wav           0\n",
       "2     cRQeAJabx0c_out.wav           0\n",
       "3     87IhSY4r2DY_out.wav           0\n",
       "4     BI4fjQBZliY_out.wav           0\n",
       "...                   ...         ...\n",
       "2626  Obc79LFQ05Q_out.wav           0\n",
       "2627  bc39DoAFVe0_out.wav           0\n",
       "2628  7K4kLUy6Kqo_out.wav           0\n",
       "2629  pk0xBpBk0s0_out.wav           0\n",
       "2630  XY94GCgWMZM_out.wav           0\n",
       "\n",
       "[2631 rows x 2 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the folder containing WAV files\n",
    "folder_path = '/kaggle/input/human-screaming-detection-dataset/NotScreaming'  # Replace with the path to your folder\n",
    "label = 'NotScreaming'  # Label for the images\n",
    "import pandas as pd\n",
    "\n",
    "# Create an empty list to store data\n",
    "predictions_data = []\n",
    "\n",
    "# Iterate through WAV files in the folder\n",
    "for i, filename in enumerate(os.listdir(folder_path)):\n",
    "    if filename.endswith('.wav'):\n",
    "        # Load the audio\n",
    "        audio, sample_rate = torchaudio.load(os.path.join(folder_path, filename))\n",
    "\n",
    "        # Transform audio to an image and save it\n",
    "        image_path = transform_data_to_image(audio, sample_rate, label, i)\n",
    "\n",
    "        # Load the saved image and apply transformations\n",
    "        image = Image.open(image_path)\n",
    "        image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "        # Make predictions using the model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(image.to(device))\n",
    "\n",
    "        predict = outputs.argmax(dim=1).cpu().detach().numpy().ravel()[0]\n",
    "\n",
    "        # Store the filename and prediction in the DataFrame\n",
    "        predictions_data.append({'Filename': filename, 'Prediction': predict})\n",
    "\n",
    "# Create a DataFrame from the list of data\n",
    "not_scream_predictions_df = pd.DataFrame(predictions_data)\n",
    "\n",
    "# Display the DataFrame with predictions\n",
    "not_scream_predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T09:00:01.836214Z",
     "iopub.status.busy": "2023-12-05T09:00:01.835520Z",
     "iopub.status.idle": "2023-12-05T09:00:01.843262Z",
     "shell.execute_reply": "2023-12-05T09:00:01.842343Z",
     "shell.execute_reply.started": "2023-12-05T09:00:01.836177Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction\n",
       "0    2596\n",
       "1      35\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_scream_predictions_df['Prediction'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
